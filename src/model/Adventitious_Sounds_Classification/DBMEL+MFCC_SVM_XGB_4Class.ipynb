{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LogMEL+MFCC_SVM_XGB_4Class.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlABfW-aDwGN",
        "colab_type": "code",
        "outputId": "9b1c8b4a-ca04-4144-d575-cf95367ea0b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import wave\n",
        "import sys\n",
        "import os\n",
        "import librosa\n",
        "import librosa.display\n",
        "import xgboost as xgb\n",
        "from  sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import GridSearchCV \n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import sklearn.naive_bayes as nb\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import BaggingClassifier,GradientBoostingClassifier\n",
        "from sklearn.neighbors import kd_tree\n",
        "import seaborn as sn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from collections import Counter\n",
        "from sklearn.datasets import make_classification\n",
        "from imblearn.over_sampling import SMOTE"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.kd_tree module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSNZcJUVEGzc",
        "colab_type": "code",
        "outputId": "521916f0-17b5-4fde-c87f-34bcc82694ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Conv1D, MaxPooling1D, Flatten\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEk_7fyfEIsy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "folder = \"/content/drive/My Drive/Respiratory_Sound_Database/Respiratory_Sound_Database/\" \n",
        "filename = folder + 'w_c_dataset.pickle'\n",
        "infile = open(filename,'rb')\n",
        "[sound,sr,lengths,times,labels] = pickle.load(infile)\n",
        "infile.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba7zWx_qEKtD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_sounds(sounds,times,labels):\n",
        "    s=[]\n",
        "    l=[]\n",
        "    for i,sound in enumerate(sounds):\n",
        "        for t,label in zip(times[i],labels[i]):\n",
        "            s.append(sound[int(t[0]):int(t[1])])\n",
        "            if label==0:\n",
        "                a=np.array([1,0,0,0])\n",
        "            if label==1:\n",
        "                a=np.array([0,1,0,0])\n",
        "            if label==2:\n",
        "                a=np.array([0,0,1,0])    \n",
        "            if label==3:\n",
        "                a=np.array([0,0,0,1])\n",
        "            l.append(a)\n",
        "    return s,l\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsGyUIC0EPP7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[data,label]=split_sounds(sound,times,labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fpu42L5iEQ4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import librosa\n",
        "\n",
        "def extract_feature(cycle_number,X,sample_rate):\n",
        "\n",
        "    mfcc = librosa.feature.mfcc(y=X, sr=sample_rate,n_mfcc=40)\n",
        "    mel_spec = librosa.feature.melspectrogram(y=X, sr=sample_rate, n_mels=40)\n",
        "    ps_db= librosa.power_to_db(mel_spec, ref=np.max)\n",
        "\n",
        "    mean_mfcc = np.mean(mfcc.T,axis=0)\n",
        "    mean_ps_db = np.mean(ps_db.T,axis=0)\n",
        "    std_mfcc = np.std(mfcc.T,axis=0)\n",
        "    std_ps_db = np.std(ps_db.T,axis=0)\n",
        "    check = np.hstack((mean_mfcc,std_mfcc,mean_ps_db,std_ps_db))\n",
        "    return check"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BoC36JRFOaK",
        "colab_type": "code",
        "outputId": "d310e62d-a8ca-4f89-bb03-3e558b4a0dd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "dataset = []\n",
        "for cycle_number,d in enumerate(data):\n",
        "    a = extract_feature(cycle_number+1,d,sr[0])\n",
        "    dataset.append(a)\n",
        "    if(cycle_number%1000 == 0):\n",
        "        print(cycle_number)\n",
        "\n",
        "\n",
        "\n",
        "data=np.asarray(dataset)\n",
        "#data = data.reshape([6898,data.shape[1]*data.shape[2],])\n",
        "print(data.shape)\n",
        "\n",
        "label=np.asarray(label)\n",
        "a=np.zeros(label.shape[0])\n",
        "for i in range(label.shape[0]):\n",
        "    for j in range(label.shape[1]):\n",
        "        if label[i][j]==1:\n",
        "            a[i]=j\n",
        "\n",
        "# COnvert to binary for the binary classification problem\n",
        "#bin_label =np.zeros(label.shape[0])\n",
        "#for index,l in enumerate(a):\n",
        " #   if l == 0:\n",
        "  #      bin_label[index] = l\n",
        "   # else:\n",
        "    #    bin_label[index] = 1\n",
        "\n",
        "unique, counts = np.unique(a, return_counts=True)\n",
        "\n",
        "print(np.asarray((unique, counts)).T)\n",
        "\n",
        "# One-hot encode labels\n",
        "le = LabelEncoder()\n",
        "i_labels = le.fit_transform(a)\n",
        "oh_labels = to_categorical(i_labels) "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "(6898, 160)\n",
            "[[0.000e+00 3.642e+03]\n",
            " [1.000e+00 1.864e+03]\n",
            " [2.000e+00 8.860e+02]\n",
            " [3.000e+00 5.060e+02]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMOup5tRFcIA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler=StandardScaler()\n",
        "x_train,x_test,y_train,y_test = sklearn.model_selection.train_test_split(dataset,i_labels,test_size=0.2, random_state=42,stratify=i_labels)\n",
        "scaler.fit(x_train)\n",
        "x_train=scaler.transform(x_train)\n",
        "x_test=scaler.transform(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Fj8YCkwGdtS",
        "colab_type": "code",
        "outputId": "7773d53a-503b-4672-f647-f7280c7aa9ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "Cs = [2**(-2),2**(-1), 1,2**(1),2**(2),2**(3),2**(4),2**(5),2**(6),2**(7),2**(8),2**(9),2**(10)]\n",
        "gamma = [2**(-7),2**(-6),2**(-5),2**(-4),2**(-3),2**(-2),2**(-1),2**(0),2**(1),2**(2),2**(3)]\n",
        "\n",
        "\n",
        "param_grid = {'C': Cs,  \n",
        "                'gamma': gamma, \n",
        "                'kernel': ['rbf'],\n",
        "                'decision_function_shape': ['ovr'],\n",
        "                'class_weight': ['balanced']}  \n",
        "\n",
        "\n",
        "\n",
        "grid1 = GridSearchCV(SVC(), param_grid,cv=3,n_jobs=-1, verbose = 3) \n",
        "\n",
        "# fitting the model for grid search \n",
        "grid1.fit(x_train, y_train)\n",
        "\n",
        "# print best parameter after tuning \n",
        "print(\"Best Parameters after tuning:\")\n",
        "print(grid1.best_params_) \n",
        "# print how our model looks after hyper-parameter tuning\n",
        "print(\"Best selected Model: \") \n",
        "print(grid1.best_estimator_)\n",
        "\n",
        "grid_predictions = grid1.predict(x_test) \n",
        "\n",
        "# print classification report\n",
        "print(\"Classification Report: \") \n",
        "print(classification_report(y_test, grid_predictions))\n",
        "print(\"Classification Accuracy: \")\n",
        "print(accuracy_score(y_test,grid_predictions))\n",
        "\n",
        "print(\"Confusion Matrix is as Follows: \")\n",
        "print(sklearn.metrics.confusion_matrix(y_test,grid_predictions))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 143 candidates, totalling 429 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:  2.2min\n",
            "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:  9.7min\n",
            "[Parallel(n_jobs=-1)]: Done 284 tasks      | elapsed: 22.5min\n",
            "[Parallel(n_jobs=-1)]: Done 429 out of 429 | elapsed: 34.2min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best Parameters after tuning:\n",
            "{'C': 16, 'class_weight': 'balanced', 'decision_function_shape': 'ovr', 'gamma': 0.015625, 'kernel': 'rbf'}\n",
            "Best selected Model: \n",
            "SVC(C=16, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=3, gamma=0.015625, kernel='rbf',\n",
            "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
            "    tol=0.001, verbose=False)\n",
            "Classification Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.85      0.84       729\n",
            "           1       0.70      0.74      0.72       373\n",
            "           2       0.74      0.63      0.68       177\n",
            "           3       0.55      0.51      0.53       101\n",
            "\n",
            "    accuracy                           0.77      1380\n",
            "   macro avg       0.70      0.68      0.69      1380\n",
            "weighted avg       0.77      0.77      0.77      1380\n",
            "\n",
            "Classification Accuracy: \n",
            "0.7666666666666667\n",
            "Confusion Matrix is as Follows: \n",
            "[[618  83  24   4]\n",
            " [ 68 276   6  23]\n",
            " [ 41   8 112  16]\n",
            " [ 10  29  10  52]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Y9vH6bqGn7G",
        "colab_type": "code",
        "outputId": "f152ff0b-90c4-429e-c205-54e235ce7b3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Running XgBoost\n",
        "import xgboost as xgb\n",
        "\n",
        "print('Training XGB Classifier from new features:')\n",
        "xgb_model = xgb.XGBClassifier(max_depth=40,num_class=4, n_estimators=60, learning_rate=0.1, colsample_bytree=.7, gamma=0, reg_alpha=4, objective='multi:softmax', eta=0.3, silent=0, subsample=0.8).fit(x_train, y_train)\n",
        "\n",
        "xgb_pred = xgb_model.predict(x_test)\n",
        "print(accuracy_score(y_test,xgb_pred))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training XGB Classifier from new features:\n",
            "0.7369565217391304\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XE05eCgaGswg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}